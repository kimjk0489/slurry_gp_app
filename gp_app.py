import streamlit as st
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from botorch.models import SingleTaskGP
from botorch.fit import fit_gpytorch_mll
from gpytorch.mlls import ExactMarginalLogLikelihood
from botorch.acquisition.analytic import LogExpectedImprovement
from botorch.optim import optimize_acqf

st.set_page_config(page_title="Carbon Black Optimization", layout="wide")
st.title("ğŸ”¬ Carbon Black ì¡°ì„± ìµœì í™” (GP + Bayesian Optimization)")

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("C:/Dev/PythonProject/slurry_data.csv")  # ê²½ë¡œ í™•ì¸

# 2. ì…ë ¥(X), ì¶œë ¥(Y) ë¶„ë¦¬
x_cols = ["carbon_black_g"]
y_cols = ["yield_stress"]
X_raw = df[x_cols].values
Y_raw = df[y_cols].values

# 3. MinMax ì •ê·œí™”
x_scaler = MinMaxScaler()
X_scaled = x_scaler.fit_transform(X_raw)

# 4. í…ì„œ ë³€í™˜
train_x = torch.tensor(X_scaled, dtype=torch.double)
train_y = torch.tensor(Y_raw, dtype=torch.double)

# 5. GP ëª¨ë¸ í•™ìŠµ
model = SingleTaskGP(train_x, train_y)
mll = ExactMarginalLogLikelihood(model.likelihood, model)
fit_gpytorch_mll(mll)

# 6. íšë“í•¨ìˆ˜ ì •ì˜
best_y = train_y.max()
acq_fn = LogExpectedImprovement(model=model, best_f=best_y, maximize=True)
bounds = torch.stack([torch.zeros(1, dtype=torch.double), torch.ones(1, dtype=torch.double)])

# 7. ìµœì  ì¡°ì„± íƒìƒ‰
candidate_scaled, _ = optimize_acqf(acq_function=acq_fn, bounds=bounds, q=1, num_restarts=5, raw_samples=20)
candidate_np = candidate_scaled.detach().numpy()
candidate_original = x_scaler.inverse_transform(candidate_np)

# 8. ì¶”ì²œ ê²°ê³¼ ì¶œë ¥
st.subheader("ğŸ“Œ ì¶”ì²œëœ carbon_black ì¡°ì„± (ë‹¨ìœ„: g)")
st.success(f"Carbon Black: {candidate_original[0][0]:.3f} g")

# 9. ì‹œê°í™”ìš© ì˜ˆì¸¡
x_vals = np.linspace(0, 1, 100)
X_test = torch.tensor(x_vals.reshape(-1, 1), dtype=torch.double)
model.eval()
with torch.no_grad():
    pred = model.posterior(X_test)
    mean = pred.mean.numpy().flatten()
    std = pred.variance.sqrt().numpy().flatten()

x_vals_g = x_scaler.inverse_transform(x_vals.reshape(-1, 1)).flatten()
train_x_g = x_scaler.inverse_transform(train_x.numpy()).flatten()
train_y_np = train_y.numpy().flatten()

# 10. ê·¸ë˜í”„ ì¶œë ¥
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(x_vals_g, mean, color="blue", label="Predicted Mean")
ax.fill_between(x_vals_g, mean - 1.96 * std, mean + 1.96 * std, color="blue", alpha=0.2, label="95% Confidence Interval")
ax.scatter(train_x_g, train_y_np, color="red", label="Observed Data")
ax.set_title("GP Prediction vs Carbon Black (unit: g)")
ax.set_xlabel("Carbon Black (g)")
ax.set_ylabel("Yield Stress")
ax.grid(True)
ax.legend()
st.pyplot(fig)
